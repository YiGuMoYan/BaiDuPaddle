{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 基于卷积神经网络实现美食分类"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import random\n",
    "import json\n",
    "import paddle\n",
    "import sys\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from PIL import ImageEnhance\n",
    "import paddle\n",
    "from paddle import fluid\n",
    "import matplotlib.pyplot as plt\n",
    "import paddle.vision.transforms as T\n",
    "import paddle.nn as nn\n",
    "import paddle.nn.functional as F"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-19T07:30:55.725807700Z",
     "start_time": "2023-07-19T07:30:53.848200700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "train_parameters = {\n",
    "    #输入图片的shape\n",
    "    \"input_size\": [3, 64, 64],\n",
    "    #分类数\n",
    "    \"class_dim\": 5,\n",
    "    #原始数据集路径\n",
    "    \"src_path\": r\"E:\\Python\\BaiDuPaddle\\基于卷积神经网络实现美食分类\\data\\foods.zip\",\n",
    "    #要解压的路径\n",
    "    \"target_path\": r\"E:\\Python\\BaiDuPaddle\\基于卷积神经网络实现美食分类\\dataset\\\\\",\n",
    "    #train.txt路径\n",
    "    \"train_list_path\": r\"E:\\Python\\BaiDuPaddle\\基于卷积神经网络实现美食分类\\train.txt\",\n",
    "    #eval.txt路径\n",
    "    \"eval_list_path\": r\"E:\\Python\\BaiDuPaddle\\基于卷积神经网络实现美食分类\\eval.txt\",\n",
    "    #readme.json路径\n",
    "    \"readme_path\": r\"E:\\Python\\BaiDuPaddle\\基于卷积神经网络实现美食分类\\readme.json\",\n",
    "    #标签字典\n",
    "    \"label_dict\": {},\n",
    "    #训练轮数\n",
    "    \"num_epochs\": 2,\n",
    "    #训练时每个批次的大小\n",
    "    \"train_batch_size\": 64,\n",
    "    #优化函数相关的配置\n",
    "    \"learning_strategy\": {\n",
    "        #超参数学习率\n",
    "        \"lr\": 0.01\n",
    "    }\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-19T07:30:55.734626Z",
     "start_time": "2023-07-19T07:30:55.725807700Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.数据准备"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def unzip_data(src_path, target_path):\n",
    "    \"\"\"\n",
    "    解压原始数据集，将src_path路径下的zip包解压至target_path目录下\n",
    "    \"\"\"\n",
    "    if not os.path.isdir(target_path + \"foods\"):\n",
    "        z = zipfile.ZipFile(src_path, 'r')\n",
    "        z.extractall(path=target_path)\n",
    "        z.close()\n",
    "\n",
    "\n",
    "def get_data_list(target_path, train_list_path, eval_list_path):\n",
    "    \"\"\"\n",
    "    生成数据列表\n",
    "    \"\"\"\n",
    "    #存放所有类别的信息\n",
    "    class_detail = []\n",
    "    #获取所有类别保存的文件夹名称\n",
    "    data_list_path = target_path + \"foods/\"\n",
    "    class_dirs = os.listdir(data_list_path)\n",
    "    #总的图像数量\n",
    "    all_class_images = 0\n",
    "    #存放类别标签\n",
    "    class_label = 0\n",
    "    #存放类别数目\n",
    "    class_dim = 0\n",
    "    #存储要写进eval.txt和train.txt中的内容\n",
    "    trainer_list = []\n",
    "    eval_list = []\n",
    "    #读取每个类别\n",
    "    for class_dir in class_dirs:\n",
    "        if class_dir != \".DS_Store\":\n",
    "            class_dim += 1\n",
    "            #每个类别的信息\n",
    "            class_detail_list = {}\n",
    "            eval_sum = 0\n",
    "            trainer_sum = 0\n",
    "            #统计每个类别有多少张图片\n",
    "            class_sum = 0\n",
    "            #获取类别路径\n",
    "            path = data_list_path + class_dir\n",
    "            # 获取所有图片\n",
    "            img_paths = os.listdir(path)\n",
    "            for img_path in img_paths:  # 遍历文件夹下的每个图片\n",
    "                name_path = path + '/' + img_path  # 每张图片的路径\n",
    "                if class_sum % 10 == 0:  # 每10张图片取一个做验证数据\n",
    "                    eval_sum += 1  # test_sum为测试数据的数目\n",
    "                    eval_list.append(name_path + \"\\t%d\" % class_label + \"\\n\")\n",
    "                else:\n",
    "                    trainer_sum += 1\n",
    "                    trainer_list.append(name_path + \"\\t%d\" % class_label + \"\\n\")  #trainer_sum测试数据的数目\n",
    "                class_sum += 1  #每类图片的数目\n",
    "                all_class_images += 1  #所有类图片的数目\n",
    "\n",
    "            # 说明的json文件的class_detail数据\n",
    "            class_detail_list['class_name'] = class_dir  #类别名称\n",
    "            class_detail_list['class_label'] = class_label  #类别标签\n",
    "            class_detail_list['class_eval_images'] = eval_sum  #该类数据的测试集数目\n",
    "            class_detail_list['class_trainer_images'] = trainer_sum  #该类数据的训练集数目\n",
    "            class_detail.append(class_detail_list)\n",
    "            #初始化标签列表\n",
    "            train_parameters['label_dict'][str(class_label)] = class_dir\n",
    "            class_label += 1\n",
    "\n",
    "            #初始化分类数\n",
    "    train_parameters['class_dim'] = class_dim\n",
    "\n",
    "    #乱序\n",
    "    random.shuffle(eval_list)\n",
    "    with open(eval_list_path, 'a') as f:\n",
    "        for eval_image in eval_list:\n",
    "            f.write(eval_image)\n",
    "\n",
    "    random.shuffle(trainer_list)\n",
    "    with open(train_list_path, 'a') as f2:\n",
    "        for train_image in trainer_list:\n",
    "            f2.write(train_image)\n",
    "\n",
    "    # 说明的json文件信息\n",
    "    readjson = {}\n",
    "    readjson['all_class_name'] = data_list_path  #文件父目录\n",
    "    readjson['all_class_images'] = all_class_images\n",
    "    readjson['class_detail'] = class_detail\n",
    "    jsons = json.dumps(readjson, sort_keys=True, indent=4, separators=(',', ': '))\n",
    "    with open(train_parameters['readme_path'], 'w') as f:\n",
    "        f.write(jsons)\n",
    "    print('生成数据列表完成！')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-19T07:30:55.744539200Z",
     "start_time": "2023-07-19T07:30:55.734626Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "生成数据列表完成！\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "参数初始化\n",
    "'''\n",
    "src_path = train_parameters['src_path']\n",
    "target_path = train_parameters['target_path']\n",
    "train_list_path = train_parameters['train_list_path']\n",
    "eval_list_path = train_parameters['eval_list_path']\n",
    "batch_size = train_parameters['train_batch_size']\n",
    "\n",
    "'''\n",
    "解压原始数据到指定路径\n",
    "'''\n",
    "unzip_data(src_path, target_path)\n",
    "\n",
    "'''\n",
    "划分训练集与验证集，乱序，生成数据列表\n",
    "'''\n",
    "#每次生成数据列表前，首先清空train.txt和eval.txt\n",
    "with open(train_list_path, 'w') as f:\n",
    "    f.seek(0)\n",
    "    f.truncate()\n",
    "with open(eval_list_path, 'w') as f:\n",
    "    f.seek(0)\n",
    "    f.truncate()\n",
    "\n",
    "#生成数据列表\n",
    "get_data_list(target_path, train_list_path, eval_list_path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-19T07:30:55.765663900Z",
     "start_time": "2023-07-19T07:30:55.745615200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "class FoodDataset(paddle.io.Dataset):\n",
    "    def __init__(self, data_path, mode='train'):\n",
    "        \"\"\"\n",
    "        数据读取器\n",
    "        :param data_path: 数据集所在路径\n",
    "        :param mode: train or eval\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.data_path = data_path\n",
    "        self.img_paths = []\n",
    "        self.labels = []\n",
    "\n",
    "        if mode == 'train':\n",
    "            with open(os.path.join(self.data_path, \"train.txt\"), \"r\", encoding=\"utf-8\") as f:\n",
    "                self.info = f.readlines()\n",
    "            for img_info in self.info:\n",
    "                img_path, label = img_info.strip().split('\\t')\n",
    "                self.img_paths.append(img_path)\n",
    "                self.labels.append(int(label))\n",
    "\n",
    "        else:\n",
    "            with open(os.path.join(self.data_path, \"eval.txt\"), \"r\", encoding=\"utf-8\") as f:\n",
    "                self.info = f.readlines()\n",
    "            for img_info in self.info:\n",
    "                img_path, label = img_info.strip().split('\\t')\n",
    "                self.img_paths.append(img_path)\n",
    "                self.labels.append(int(label))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        获取一组数据\n",
    "        :param index: 文件索引号\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # 第一步打开图像文件并获取label值\n",
    "        img_path = self.img_paths[index]\n",
    "        img = Image.open(img_path)\n",
    "        if img.mode != 'RGB':\n",
    "            img = img.convert('RGB')\n",
    "        img = img.resize((64, 64), Image.BILINEAR)\n",
    "        img = np.array(img).astype('float32')\n",
    "        img = img.transpose((2, 0, 1)) / 255\n",
    "        label = self.labels[index]\n",
    "        label = np.array([label], dtype=\"int64\")\n",
    "        return img, label\n",
    "\n",
    "    def print_sample(self, index: int = 0):\n",
    "        print(\"文件名\", self.img_paths[index], \"\\t标签值\", self.labels[index])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-19T07:30:55.774412700Z",
     "start_time": "2023-07-19T07:30:55.765663900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xbb in position 22: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mUnicodeDecodeError\u001B[0m                        Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[7], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m train_dataset \u001B[38;5;241m=\u001B[39m \u001B[43mFoodDataset\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m./\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtrain\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m      2\u001B[0m eval_dataset \u001B[38;5;241m=\u001B[39m FoodDataset(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m./\u001B[39m\u001B[38;5;124m'\u001B[39m, mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124meval\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m      4\u001B[0m train_loader \u001B[38;5;241m=\u001B[39m paddle\u001B[38;5;241m.\u001B[39mio\u001B[38;5;241m.\u001B[39mDataLoader(train_dataset, batch_size\u001B[38;5;241m=\u001B[39mtrain_parameters[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrain_batch_size\u001B[39m\u001B[38;5;124m\"\u001B[39m], shuffle\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "Cell \u001B[1;32mIn[6], line 15\u001B[0m, in \u001B[0;36mFoodDataset.__init__\u001B[1;34m(self, data_path, mode)\u001B[0m\n\u001B[0;32m     13\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m mode \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtrain\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[0;32m     14\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdata_path, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrain.txt\u001B[39m\u001B[38;5;124m\"\u001B[39m), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124m\"\u001B[39m, encoding\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mutf-8\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m f:\n\u001B[1;32m---> 15\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minfo \u001B[38;5;241m=\u001B[39m \u001B[43mf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreadlines\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     16\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m img_info \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minfo:\n\u001B[0;32m     17\u001B[0m         img_path, label \u001B[38;5;241m=\u001B[39m img_info\u001B[38;5;241m.\u001B[39mstrip()\u001B[38;5;241m.\u001B[39msplit(\u001B[38;5;124m'\u001B[39m\u001B[38;5;130;01m\\t\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[1;32mG:\\Anaconda3\\envs\\paddle\\lib\\codecs.py:322\u001B[0m, in \u001B[0;36mBufferedIncrementalDecoder.decode\u001B[1;34m(self, input, final)\u001B[0m\n\u001B[0;32m    319\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecode\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m, final\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m):\n\u001B[0;32m    320\u001B[0m     \u001B[38;5;66;03m# decode input (taking the buffer into account)\u001B[39;00m\n\u001B[0;32m    321\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbuffer \u001B[38;5;241m+\u001B[39m \u001B[38;5;28minput\u001B[39m\n\u001B[1;32m--> 322\u001B[0m     (result, consumed) \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_buffer_decode\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43merrors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfinal\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    323\u001B[0m     \u001B[38;5;66;03m# keep undecoded input until the next call\u001B[39;00m\n\u001B[0;32m    324\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbuffer \u001B[38;5;241m=\u001B[39m data[consumed:]\n",
      "\u001B[1;31mUnicodeDecodeError\u001B[0m: 'utf-8' codec can't decode byte 0xbb in position 22: invalid start byte"
     ]
    }
   ],
   "source": [
    "train_dataset = FoodDataset('./', mode='train')\n",
    "eval_dataset = FoodDataset('./', mode='eval')\n",
    "\n",
    "train_loader = paddle.io.DataLoader(train_dataset, batch_size=train_parameters[\"train_batch_size\"], shuffle=True)\n",
    "eval_loader = paddle.io.DataLoader(eval_dataset, batch_size=train_parameters[\"train_batch_size\"], shuffle=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-19T07:30:55.854583800Z",
     "start_time": "2023-07-19T07:30:55.774412700Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 模型搭建"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class MyCNN(paddle.nn.Layer):\n",
    "    def __init__(self):\n",
    "        super(MyCNN, self).__init__()\n",
    "        # 62 * 62\n",
    "        self.conv1 = nn.Conv2D(in_channels=3, out_channels=64, kernel_size=3, padding=0, stride=1)\n",
    "        # 31 * 31\n",
    "        self.pool1 = nn.MaxPool2D(kernel_size=2, stride=2)\n",
    "        # 29 * 29\n",
    "        self.conv2 = nn.Conv2D(in_channels=64, out_channels=128, kernel_size=3, padding=0, stride=1)\n",
    "        # 14 * 14\n",
    "        self.pool2 = nn.MaxPool2D(kernel_size=2, stride=2)\n",
    "        # 10 * 10\n",
    "        self.conv3 = nn.Conv2D(in_channels=128, out_channels=128, kernel_size=5, padding=0, stride=1)\n",
    "        # 5 * 5\n",
    "        self.pool3 = nn.MaxPool2D(kernel_size=2, stride=2)\n",
    "        self.fc = nn.Linear(in_features=5 * 5 * 128, out_features=5)\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = self.conv1(input)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.pool3(x)\n",
    "        x = paddle.reshape(x, [-1, 5 * 5 * 128])\n",
    "        x = self.fc(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 实例化网络\n",
    "model = MyCNN()\n",
    "\n",
    "# 定义输入\n",
    "input_define = paddle.static.InputSpec(shape=[-1, 3, 64, 64],\n",
    "                                       dtype=\"float32\",\n",
    "                                       name=\"img\")\n",
    "\n",
    "label_define = paddle.static.InputSpec(shape=[-1, 1],\n",
    "                                       dtype=\"int64\",\n",
    "                                       name=\"label\")\n",
    "model = paddle.Model(model, inputs=input_define, labels=label_define)\n",
    "params_info = model.summary((1, 3, 64, 64))\n",
    "\n",
    "# 打印模型基础结构和参数信息\n",
    "print(params_info)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.模型训练"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Batch = 0\n",
    "Batchs = []\n",
    "all_train_accs = []\n",
    "\n",
    "\n",
    "def draw_train_acc(Batchs, train_accs):\n",
    "    title = \"training accs\"\n",
    "    plt.title(title, fontsize=24)\n",
    "    plt.xlabel(\"batch\", fontsize=14)\n",
    "    plt.ylabel(\"acc\", fontsize=14)\n",
    "    plt.plot(Batchs, train_accs, color='green', label='training accs')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "all_train_loss = []\n",
    "\n",
    "\n",
    "def draw_train_loss(Batchs, train_loss):\n",
    "    title = \"training loss\"\n",
    "    plt.title(title, fontsize=24)\n",
    "    plt.xlabel(\"batch\", fontsize=14)\n",
    "    plt.ylabel(\"loss\", fontsize=14)\n",
    "    plt.plot(Batchs, train_loss, color='red', label='training loss')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = MyCNN()\n",
    "model.train()\n",
    "cross_entropy = paddle.nn.CrossEntropyLoss()\n",
    "opt = paddle.optimizer.SGD(learning_rate=train_parameters[\"learning_strategy\"][\"lr\"], parameters=model.parameters())\n",
    "epoch_num = train_parameters[\"num_epochs\"]\n",
    "\n",
    "for epoch in range(epoch_num):\n",
    "    for batch_id, data in enumerate(train_loader):\n",
    "        img = data[0]\n",
    "        label = data[1]\n",
    "        predict = model(img)\n",
    "        loss = cross_entropy(predict, label)\n",
    "        acc = paddle.metric.accuracy(predict, label.reshape([-1, 1]))\n",
    "\n",
    "        if batch_id != 0 and batch_id % 10 == 0:\n",
    "            Batch = Batch + 10\n",
    "            Batchs.append(Batch)\n",
    "            all_train_loss.append(loss.numpy()[0])\n",
    "            all_train_accs.append(acc.numpy()[0])\n",
    "            print(\"epoch:{},step:{},train_loss:{},train_acc:{}\".format(epoch, batch_id, loss.numpy()[0],\n",
    "                                                                       acc.numpy()[0]))\n",
    "\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.clear_grad()\n",
    "\n",
    "paddle.save(model.state_dict(), 'MyCNN')  #保存模型\n",
    "draw_train_acc(Batchs, all_train_accs)\n",
    "draw_train_loss(Batchs, all_train_loss)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5.模型评估"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#模型评估\n",
    "para_state_dict = paddle.load(\"MyCNN\")\n",
    "model = MyCNN()\n",
    "model.set_state_dict(para_state_dict)  #加载模型参数\n",
    "model.eval()  #验证模式\n",
    "\n",
    "accs = []\n",
    "\n",
    "for batch_id, data in enumerate(eval_loader()):  #测试集\n",
    "    image = data[0]\n",
    "    label = data[1]\n",
    "    predict = model(image)\n",
    "    acc = paddle.metric.accuracy(predict, label)\n",
    "    accs.append(acc.numpy()[0])\n",
    "    avg_acc = np.mean(accs)\n",
    "print(\"当前模型在验证集上的准确率为:\", avg_acc)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
