{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 基于LSTM实现谣言检测"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import paddle\n",
    "\n",
    "print(paddle.__version__)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 数据准备"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os, zipfile\n",
    "\n",
    "src_path = r\"E:\\Python\\BaiDuPaddle\\基于LSTM实现谣言检测\\data\\Rumor_Dataset.zip\"\n",
    "target_path = r\"E:\\Python\\BaiDuPaddle\\基于LSTM实现谣言检测\\data\\dataset\"\n",
    "if (not os.path.isdir(target_path)):\n",
    "    z = zipfile.ZipFile(src_path, 'r')\n",
    "    z.extractall(path=target_path)\n",
    "    z.close()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import io\n",
    "import random\n",
    "import json\n",
    "\n",
    "#谣言数据文件路径\n",
    "rumor_class_dirs = os.listdir(target_path + r\"/Chinese_Rumor_Dataset-master/CED_Dataset/rumor-repost/\")\n",
    "\n",
    "#非谣言数据文件路径\n",
    "non_rumor_class_dirs = os.listdir(target_path + r\"/Chinese_Rumor_Dataset-master/CED_Dataset/non-rumor-repost/\")\n",
    "\n",
    "original_microblog = target_path + r\"/Chinese_Rumor_Dataset-master/CED_Dataset/original-microblog/\"\n",
    "\n",
    "#谣言标签为0，非谣言标签为1\n",
    "rumor_label = \"0\"\n",
    "non_rumor_label = \"1\"\n",
    "\n",
    "#分别统计谣言数据与非谣言数据的总数\n",
    "rumor_num = 0\n",
    "non_rumor_num = 0\n",
    "\n",
    "all_rumor_list = []\n",
    "all_non_rumor_list = []\n",
    "\n",
    "#解析谣言数据\n",
    "for rumor_class_dir in rumor_class_dirs:\n",
    "    if (rumor_class_dir != '.DS_Store'):\n",
    "        #遍历谣言数据，并解析\n",
    "        with open(original_microblog + rumor_class_dir, 'r', encoding='utf8') as f:\n",
    "            rumor_content = f.read()\n",
    "        rumor_dict = json.loads(rumor_content)\n",
    "        all_rumor_list.append(rumor_label + \"\\t\" + rumor_dict[\"text\"] + \"\\n\")\n",
    "        rumor_num += 1\n",
    "\n",
    "#解析非谣言数据\n",
    "for non_rumor_class_dir in non_rumor_class_dirs:\n",
    "    if (non_rumor_class_dir != '.DS_Store'):\n",
    "        with open(original_microblog + non_rumor_class_dir, 'r', encoding='utf8') as f2:\n",
    "            non_rumor_content = f2.read()\n",
    "        non_rumor_dict = json.loads(non_rumor_content)\n",
    "        all_non_rumor_list.append(non_rumor_label + \"\\t\" + non_rumor_dict[\"text\"] + \"\\n\")\n",
    "        non_rumor_num += 1\n",
    "\n",
    "print(\"谣言数据总量为：\" + str(rumor_num))\n",
    "print(\"非谣言数据总量为：\" + str(non_rumor_num))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#全部数据进行乱序后写入all_data.txt\n",
    "\n",
    "data_list_path = \"E:\\Python\\BaiDuPaddle\\基于LSTM实现谣言检测\\data/\"\n",
    "all_data_path = data_list_path + \"all_data.txt\"\n",
    "\n",
    "all_data_list = all_rumor_list + all_non_rumor_list\n",
    "\n",
    "random.shuffle(all_data_list)\n",
    "\n",
    "#在生成all_data.txt之前，首先将其清空\n",
    "with open(all_data_path, 'w', encoding='utf8') as f:\n",
    "    f.seek(0)\n",
    "    f.truncate()\n",
    "\n",
    "with open(all_data_path, 'a', encoding='utf8') as f:\n",
    "    for data in all_data_list:\n",
    "        f.write(data)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 生成数据字典\n",
    "def create_dict(data_path, dict_path):\n",
    "    with open(dict_path, 'w', encoding='utf8') as f:\n",
    "        f.seek(0)\n",
    "        f.truncate()\n",
    "\n",
    "    dict_set = set()\n",
    "    # 读取全部数据\n",
    "    with open(data_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    # 把数据生成一个元组\n",
    "    for line in lines:\n",
    "        content = line.split('\\t')[-1].replace('\\n', '')\n",
    "        for s in content:\n",
    "            dict_set.add(s)\n",
    "    # 把元组转换成字典，一个字对应一个数字\n",
    "    dict_list = []\n",
    "    i = 0\n",
    "    for s in dict_set:\n",
    "        dict_list.append([s, i])\n",
    "        i += 1\n",
    "    # 添加未知字符\n",
    "    dict_txt = dict(dict_list)\n",
    "    end_dict = {\"<unk>\": i}\n",
    "    dict_txt.update(end_dict)\n",
    "    end_dict = {\"<pad>\": i + 1}\n",
    "    dict_txt.update(end_dict)\n",
    "    # 把这些字典保存到本地中\n",
    "    with open(dict_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(str(dict_txt))\n",
    "\n",
    "    print(\"数据字典生成完成！\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 创建序列化表示的数据,并按照一定比例划分训练数据train_list.txt与验证数据eval_list.txt\n",
    "def create_data_list(data_list_path):\n",
    "    #在生成数据之前，首先将eval_list.txt和train_list.txt清空\n",
    "    with open(os.path.join(data_list_path, 'eval_list.txt'), 'w', encoding='utf-8') as f_eval:\n",
    "        f_eval.seek(0)\n",
    "        f_eval.truncate()\n",
    "\n",
    "    with open(os.path.join(data_list_path, 'train_list.txt'), 'w', encoding='utf-8') as f_train:\n",
    "        f_train.seek(0)\n",
    "        f_train.truncate()\n",
    "\n",
    "    with open(os.path.join(data_list_path, 'dict.txt'), 'r', encoding='utf-8') as f_data:\n",
    "        dict_txt = eval(f_data.readlines()[0])\n",
    "\n",
    "    with open(os.path.join(data_list_path, 'all_data.txt'), 'r', encoding='utf-8') as f_data:\n",
    "        lines = f_data.readlines()\n",
    "\n",
    "    i = 0\n",
    "    maxlen = 0\n",
    "    with open(os.path.join(data_list_path, 'eval_list.txt'), 'a', encoding='utf-8') as f_eval, open(\n",
    "            os.path.join(data_list_path, 'train_list.txt'), 'a', encoding='utf-8') as f_train:\n",
    "        for line in lines:\n",
    "            words = line.split('\\t')[-1].replace('\\n', '')\n",
    "            maxlen = max(maxlen, len(words))\n",
    "            label = line.split('\\t')[0]\n",
    "            labs = \"\"\n",
    "            # 每8个 抽取一个数据用于验证\n",
    "            if i % 8 == 0:\n",
    "                for s in words:\n",
    "                    lab = str(dict_txt[s])\n",
    "                    labs = labs + lab + ','\n",
    "                labs = labs[:-1]\n",
    "                labs = labs + '\\t' + label + '\\n'\n",
    "                f_eval.write(labs)\n",
    "            else:\n",
    "                for s in words:\n",
    "                    lab = str(dict_txt[s])\n",
    "                    labs = labs + lab + ','\n",
    "                labs = labs[:-1]\n",
    "                labs = labs + '\\t' + label + '\\n'\n",
    "                f_train.write(labs)\n",
    "            i += 1\n",
    "\n",
    "    print(\"数据列表生成完成！\")\n",
    "    print(\"样本最长长度：\" + str(maxlen))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 把生成的数据列表都放在自己的总类别文件夹中\n",
    "data_root_path = \"E:\\Python\\BaiDuPaddle\\基于LSTM实现谣言检测\\data\"\n",
    "data_path = os.path.join(data_root_path, 'all_data.txt')\n",
    "dict_path = os.path.join(data_root_path, \"dict.txt\")\n",
    "\n",
    "# 创建数据字典\n",
    "create_dict(data_path, dict_path)\n",
    "\n",
    "# 创建数据列表\n",
    "create_data_list(data_root_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def load_vocab(file_path):\n",
    "    fr = open(file_path, 'r', encoding='utf8')\n",
    "    vocab = eval(fr.read())  #读取的str转换为字典\n",
    "    fr.close()\n",
    "\n",
    "    return vocab"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 打印前2条训练数据\n",
    "vocab = load_vocab(os.path.join(data_root_path, 'dict.txt'))\n",
    "\n",
    "\n",
    "def ids_to_str(ids):\n",
    "    words = []\n",
    "    for k in ids:\n",
    "        w = list(vocab.keys())[list(vocab.values()).index(int(k))]\n",
    "        words.append(w if isinstance(w, str) else w.decode('ASCII'))\n",
    "    return \" \".join(words)\n",
    "\n",
    "\n",
    "file_path = os.path.join(data_root_path, 'train_list.txt')\n",
    "with io.open(file_path, \"r\", encoding='utf8') as fin:\n",
    "    i = 0\n",
    "    for line in fin:\n",
    "        i += 1\n",
    "        cols = line.strip().split(\"\\t\")\n",
    "        if len(cols) != 2:\n",
    "            sys.stderr.write(\"[NOTICE] Error Format Line!\")\n",
    "            continue\n",
    "        label = int(cols[1])\n",
    "        wids = cols[0].split(\",\")\n",
    "        print(str(i) + \":\")\n",
    "        print('sentence list id is:', wids)\n",
    "        print('sentence list is: ', ids_to_str(wids))\n",
    "        print('sentence label id is:', label)\n",
    "        print('---------------------------------')\n",
    "\n",
    "        if i == 2: break"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "vocab = load_vocab(os.path.join(data_root_path, 'dict.txt'))\n",
    "\n",
    "\n",
    "class RumorDataset(paddle.io.Dataset):\n",
    "    def __init__(self, data_dir):\n",
    "        self.data_dir = data_dir\n",
    "        self.all_data = []\n",
    "\n",
    "        with io.open(self.data_dir, \"r\", encoding='utf8') as fin:\n",
    "            for line in fin:\n",
    "                cols = line.strip().split(\"\\t\")\n",
    "                if len(cols) != 2:\n",
    "                    sys.stderr.write(\"[NOTICE] Error Format Line!\")\n",
    "                    continue\n",
    "                label = []\n",
    "                label.append(int(cols[1]))\n",
    "                wids = cols[0].split(\",\")\n",
    "                if len(wids) >= 150:\n",
    "                    wids = np.array(wids[:150]).astype('int64')\n",
    "                else:\n",
    "                    wids = np.concatenate([wids, [vocab[\"<pad>\"]] * (150 - len(wids))]).astype('int64')\n",
    "                label = np.array(label).astype('int64')\n",
    "                self.all_data.append((wids, label))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data, label = self.all_data[index]\n",
    "        return data, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_data)\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "train_dataset = RumorDataset(os.path.join(data_root_path, 'train_list.txt'))\n",
    "test_dataset = RumorDataset(os.path.join(data_root_path, 'eval_list.txt'))\n",
    "\n",
    "train_loader = paddle.io.DataLoader(train_dataset, places=paddle.CPUPlace(), return_list=True,\n",
    "                                    shuffle=True, batch_size=batch_size, drop_last=True)\n",
    "test_loader = paddle.io.DataLoader(test_dataset, places=paddle.CPUPlace(), return_list=True,\n",
    "                                   shuffle=True, batch_size=batch_size, drop_last=True)\n",
    "\n",
    "#check\n",
    "\n",
    "print('=============train_dataset =============')\n",
    "for data, label in train_dataset:\n",
    "    print(data)\n",
    "    print(np.array(data).shape)\n",
    "    print(label)\n",
    "    break\n",
    "\n",
    "print('=============test_dataset =============')\n",
    "for data, label in test_dataset:\n",
    "    print(data)\n",
    "    print(np.array(data).shape)\n",
    "    print(label)\n",
    "    break\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.模型配置"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import paddle\n",
    "from paddle.nn import Conv2D, Linear, Embedding\n",
    "from paddle import to_tensor\n",
    "import paddle.nn.functional as F\n",
    "\n",
    "class RNN(paddle.nn.Layer):\n",
    "    def __init__(self):\n",
    "        super(RNN, self).__init__()\n",
    "        self.dict_dim = vocab[\"<pad>\"]\n",
    "        self.emb_dim = 128\n",
    "        self.hid_dim = 128\n",
    "        self.class_dim = 2\n",
    "        self.embedding = Embedding(\n",
    "            self.dict_dim + 1, self.emb_dim,\n",
    "            sparse=False)\n",
    "        self._fc1 = Linear(self.emb_dim, self.hid_dim)\n",
    "        self.lstm = paddle.nn.LSTM(self.hid_dim, self.hid_dim)\n",
    "        self.fc2 = Linear(19200, self.class_dim)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # [32, 150]\n",
    "        emb = self.embedding(inputs)\n",
    "        # [32, 150, 128]\n",
    "        fc_1 = self._fc1(emb)\n",
    "        # [32, 150, 128]\n",
    "        x = self.lstm(fc_1)\n",
    "        x = paddle.reshape(x[0], [0, -1])\n",
    "        x = self.fc2(x)\n",
    "        x = paddle.nn.functional.softmax(x)\n",
    "        return x\n",
    "\n",
    "rnn = RNN()\n",
    "paddle.summary(rnn,(32,150),\"int64\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.模型训练"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "def draw_process(title,color,iters,data,label):\n",
    "    plt.title(title, fontsize=24)\n",
    "    plt.xlabel(\"iter\", fontsize=20)\n",
    "    plt.ylabel(label, fontsize=20)\n",
    "    plt.plot(iters, data,color=color,label=label)\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train(model):\n",
    "    model.train()\n",
    "    opt = paddle.optimizer.Adam(learning_rate=0.002, parameters=model.parameters())\n",
    "\n",
    "    steps = 0\n",
    "    Iters, total_loss, total_acc = [], [], []\n",
    "\n",
    "    for epoch in range(3):\n",
    "        for batch_id, data in enumerate(train_loader):\n",
    "            steps += 1\n",
    "            sent = data[0]\n",
    "            label = data[1]\n",
    "\n",
    "            logits = model(sent)\n",
    "            loss = paddle.nn.functional.cross_entropy(logits, label)\n",
    "            acc = paddle.metric.accuracy(logits, label)\n",
    "\n",
    "            if batch_id % 50 == 0:\n",
    "                Iters.append(steps)\n",
    "                total_loss.append(loss.numpy()[0])\n",
    "                total_acc.append(acc.numpy()[0])\n",
    "\n",
    "                print(\"epoch: {}, batch_id: {}, loss is: {}\".format(epoch, batch_id, loss.numpy()))\n",
    "\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.clear_grad()\n",
    "\n",
    "        # evaluate model after one epoch\n",
    "        model.eval()\n",
    "        accuracies = []\n",
    "        losses = []\n",
    "\n",
    "        for batch_id, data in enumerate(test_loader):\n",
    "\n",
    "            sent = data[0]\n",
    "            label = data[1]\n",
    "\n",
    "            logits = model(sent)\n",
    "            loss = paddle.nn.functional.cross_entropy(logits, label)\n",
    "            acc = paddle.metric.accuracy(logits, label)\n",
    "\n",
    "            accuracies.append(acc.numpy())\n",
    "            losses.append(loss.numpy())\n",
    "\n",
    "        avg_acc, avg_loss = np.mean(accuracies), np.mean(losses)\n",
    "\n",
    "        print(\"[validation] accuracy: {}, loss: {}\".format(avg_acc, avg_loss))\n",
    "\n",
    "        model.train()\n",
    "\n",
    "    paddle.save(model.state_dict(),\"model_final.pdparams\")\n",
    "\n",
    "    draw_process(\"trainning loss\",\"red\",Iters,total_loss,\"trainning loss\")\n",
    "    draw_process(\"trainning acc\",\"green\",Iters,total_acc,\"trainning acc\")\n",
    "\n",
    "model = RNN()\n",
    "train(model)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.模型评估"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "'''\n",
    "模型评估\n",
    "'''\n",
    "model_state_dict = paddle.load('model_final.pdparams')\n",
    "model = RNN()\n",
    "model.set_state_dict(model_state_dict)\n",
    "model.eval()\n",
    "label_map = {0:\"是\", 1:\"否\"}\n",
    "samples = []\n",
    "predictions = []\n",
    "accuracies = []\n",
    "losses = []\n",
    "\n",
    "for batch_id, data in enumerate(test_loader):\n",
    "\n",
    "    sent = data[0]\n",
    "    label = data[1]\n",
    "\n",
    "    logits = model(sent)\n",
    "\n",
    "    for idx,probs in enumerate(logits):\n",
    "        # 映射分类label\n",
    "        label_idx = np.argmax(probs)\n",
    "        labels = label_map[label_idx]\n",
    "        predictions.append(labels)\n",
    "        samples.append(sent[idx].numpy())\n",
    "\n",
    "    loss = paddle.nn.functional.cross_entropy(logits, label)\n",
    "    acc = paddle.metric.accuracy(logits, label)\n",
    "\n",
    "    accuracies.append(acc.numpy())\n",
    "    losses.append(loss.numpy())\n",
    "\n",
    "avg_acc, avg_loss = np.mean(accuracies), np.mean(losses)\n",
    "print(\"[validation] accuracy: {}, loss: {}\".format(avg_acc, avg_loss))\n",
    "print('数据: {} \\n\\n是否谣言: {}'.format(ids_to_str(samples[0]), predictions[0]))"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
